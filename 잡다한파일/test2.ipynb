{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as random\n",
    "import tensorflowjs as tfjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GomokuBoard:\n",
    "    def __init__(self, size=15):\n",
    "        self.size = size\n",
    "        self.board = np.zeros((size, size), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def make_move(self, row, col):\n",
    "        if self.board[row, col] == 0:\n",
    "            self.board[row, col] = self.current_player\n",
    "            self.current_player = 3 - self.current_player\n",
    "            return True\n",
    "        return False  # 이미 돌이 놓인 자리면 False 반환\n",
    "\n",
    "    def check_win(self, player):\n",
    "        # 간단한 승리 체크 (가로, 세로, 대각선)\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (j <= self.size - 5 and np.all(self.board[i, j:j+5] == player) or\n",
    "                    i <= self.size - 5 and np.all(self.board[i:i+5, j] == player) or\n",
    "                    i <= self.size - 5 and j <= self.size - 5 and np.all(np.diagonal(self.board[i:i+5, j:j+5]) == player) or\n",
    "                    i <= self.size - 5 and j >= 4 and np.all(np.diagonal(np.fliplr(self.board[i:i+5, j-4:j+1])) == player)):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def calculate_reward(self, row, col, player):\n",
    "        reward = 0\n",
    "\n",
    "        # 승리 체크\n",
    "        if self.check_win(player):\n",
    "            return 1.0  # 승리 시 최대 보상\n",
    "\n",
    "        # 유효한 수 보상\n",
    "        if self.board[row, col] == player:\n",
    "            reward += 0.1\n",
    "\n",
    "        # 연속된 돌에 대한 보상\n",
    "        directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "        for dr, dc in directions:\n",
    "            count = 1\n",
    "            for i in range(1, 5):\n",
    "                r, c = row + dr * i, col + dc * i\n",
    "                if 0 <= r < self.size and 0 <= c < self.size and self.board[r, c] == player:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            for i in range(1, 5):\n",
    "                r, c = row - dr * i, col - dc * i\n",
    "                if 0 <= r < self.size and 0 <= c < self.size and self.board[r, c] == player:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            reward += self.get_sequence_reward(count)\n",
    "\n",
    "        # 중앙 근처에 둔 경우 추가 보상\n",
    "        center = self.size // 2\n",
    "        distance_to_center = abs(row - center) + abs(col - center)\n",
    "        reward += max(0, (self.size - distance_to_center) / self.size * 0.1)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_sequence_reward(self, count):\n",
    "        if count == 2:\n",
    "            return 0.01\n",
    "        elif count == 3:\n",
    "            return 0.05\n",
    "        elif count == 4:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        return [(i, j) for i in range(self.size) for j in range(self.size) if self.board[i, j] == 0]\n",
    "\n",
    "\n",
    "# AI 모델\n",
    "def create_model(board_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(board_size, board_size, 1)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(board_size * board_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# 학습 함수\n",
    "def train_model(model, num_episodes=1):\n",
    "    board = GomokuBoard()\n",
    "    epsilon = 0.3  # 탐험 확률\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = board.board.reshape(board.size, board.size, 1)\n",
    "        done = False\n",
    "        move_history = set()  # 이번 게임에서 둔 수를 기록\n",
    "        while not done:\n",
    "            valid_moves = board.get_valid_moves()\n",
    "            if not valid_moves:  # 더 이상 둘 수 있는 곳이 없으면 무승부\n",
    "                done = True\n",
    "                continue\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                # 탐험: 랜덤하게 유효한 수 선택\n",
    "                row, col = random.choice(valid_moves)\n",
    "                action = row * board.size + col\n",
    "            else:\n",
    "                # 활용: 모델이 예측한 최선의 수 선택\n",
    "                predictions = model.predict(np.array([state]))[0]\n",
    "                valid_actions = [r * board.size + c for r, c in valid_moves]\n",
    "                action = max(valid_actions, key=lambda a: predictions[a])\n",
    "\n",
    "            row, col = action // board.size, action % board.size\n",
    "            \n",
    "            # 유효한 수인지 다시 한번 확인\n",
    "            if (row, col) not in valid_moves:\n",
    "                continue\n",
    "\n",
    "            prev_player = board.current_player\n",
    "            move_success = board.make_move(row, col)\n",
    "            \n",
    "            if not move_success:  # 이미 돌이 놓인 자리라면 다시 선택\n",
    "                continue\n",
    "\n",
    "            move_history.add((row, col))\n",
    "            \n",
    "            reward = board.calculate_reward(row, col, prev_player)\n",
    "            new_state = board.board.reshape(board.size, board.size, 1)\n",
    "            \n",
    "            if board.check_win(prev_player):\n",
    "                done = True\n",
    "                reward = 1.0  # 승리 시 최대 보상\n",
    "\n",
    "            # Q-learning 업데이트\n",
    "            target = reward + 0.99 * np.max(model.predict(np.array([new_state]))[0])\n",
    "            target_vec = model.predict(np.array([state]))[0]\n",
    "            target_vec[action] = target\n",
    "            model.fit(np.array([state]), np.array([target_vec]), epochs=1, verbose=0)\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            if len(move_history) == board.size * board.size:  # 모든 칸이 채워졌을 때\n",
    "                done = True\n",
    "                \n",
    "\n",
    "        # 에피소드 종료 후 보드 초기화\n",
    "        board = GomokuBoard()\n",
    "\n",
    "        # 탐험 확률 감소\n",
    "        epsilon = max(0.01, epsilon * 0.995)\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_web(model, filepath):\n",
    "    # 네이티브 Keras 형식으로 저장\n",
    "    model.save('temp_model.keras', save_format='keras')\n",
    "    \n",
    "    # Keras 모델을 TensorFlow.js 형식으로 변환\n",
    "    tfjs.converters.save_keras_model(tf.keras.models.load_model('temp_model.keras'), filepath)\n",
    "    \n",
    "    print(f\"Model saved for web at {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n",
      "Model saved for web at web_model\n"
     ]
    }
   ],
   "source": [
    "save_model_for_web(model, \"web_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
